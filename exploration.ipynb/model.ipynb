{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)  # Token embeddings\n",
    "        self.pos_emb = nn.Embedding(1000, d_model)          # Positional embeddings (max sequence length = 1000)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)  # (1, seq_length)\n",
    "        token_emb = self.token_emb(x)  # (batch_size, seq_length, d_model)\n",
    "        pos_emb = self.pos_emb(positions)  # (1, seq_length, d_model)\n",
    "        return token_emb + pos_emb  # Add and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)  # Query\n",
    "        self.wk = nn.Linear(d_model, d_model)  # Key\n",
    "        self.wv = nn.Linear(d_model, d_model)  # Value\n",
    "        self.fc = nn.Linear(d_model, d_model)  # Final projection\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "        # Concatenate heads and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(torch.relu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Self-attention with residual\n",
    "        attn_out = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        # Feed-forward with residual\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embeddings(x)  # (batch_size, seq_length, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.fc(x)  # (batch_size, seq_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniGPT(\n",
      "  (embeddings): Embeddings(\n",
      "    (token_emb): Embedding(85, 128)\n",
      "    (pos_emb): Embedding(1000, 128)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x DecoderLayer(\n",
      "      (self_attn): MultiHeadAttention(\n",
      "        (wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=85, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load vocabulary\n",
    "with open(\"/home/itachi/Mini-GPT/data/processed/vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "    char_to_idx = vocab[\"char_to_idx\"]  # Now this is defined!\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(char_to_idx)  # Correctly set using the loaded vocabulary\n",
    "d_model = 128                  \n",
    "num_layers = 4                 \n",
    "num_heads = 8                  \n",
    "d_ff = 512                     \n",
    "dropout = 0.1                  \n",
    "max_seq_len = 64               \n",
    "\n",
    "# Initialize model\n",
    "model = MiniGPT(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_len)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "def generate_mask(seq_len):\n",
    "    return torch.triu(torch.ones(seq_len, seq_len) == 1).transpose(0, 1)\n",
    "\n",
    "# Example\n",
    "mask = generate_mask(10)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Batch 0/2809 | Loss: 4.7416\n",
      "Epoch 1/1 | Batch 100/2809 | Loss: 2.5177\n",
      "Epoch 1/1 | Batch 200/2809 | Loss: 2.3827\n",
      "Epoch 1/1 | Batch 300/2809 | Loss: 2.2945\n",
      "Epoch 1/1 | Batch 400/2809 | Loss: 2.1976\n",
      "Epoch 1/1 | Batch 500/2809 | Loss: 2.2060\n",
      "Epoch 1/1 | Batch 600/2809 | Loss: 2.0961\n",
      "Epoch 1/1 | Batch 700/2809 | Loss: 2.0302\n",
      "Epoch 1/1 | Batch 800/2809 | Loss: 1.9742\n",
      "Epoch 1/1 | Batch 900/2809 | Loss: 1.9529\n",
      "Epoch 1/1 | Batch 1000/2809 | Loss: 1.9492\n",
      "Epoch 1/1 | Batch 1100/2809 | Loss: 1.8726\n",
      "Epoch 1/1 | Batch 1200/2809 | Loss: 1.7702\n",
      "Epoch 1/1 | Batch 1300/2809 | Loss: 1.8927\n",
      "Epoch 1/1 | Batch 1400/2809 | Loss: 1.7962\n",
      "Epoch 1/1 | Batch 1500/2809 | Loss: 1.7667\n",
      "Epoch 1/1 | Batch 1600/2809 | Loss: 1.7359\n",
      "Epoch 1/1 | Batch 1700/2809 | Loss: 1.7419\n",
      "Epoch 1/1 | Batch 1800/2809 | Loss: 1.7145\n",
      "Epoch 1/1 | Batch 1900/2809 | Loss: 1.7221\n",
      "Epoch 1/1 | Batch 2000/2809 | Loss: 1.6721\n",
      "Epoch 1/1 | Batch 2100/2809 | Loss: 1.6228\n",
      "Epoch 1/1 | Batch 2200/2809 | Loss: 1.6427\n",
      "Epoch 1/1 | Batch 2300/2809 | Loss: 1.6663\n",
      "Epoch 1/1 | Batch 2400/2809 | Loss: 1.5482\n",
      "Epoch 1/1 | Batch 2500/2809 | Loss: 1.5555\n",
      "Epoch 1/1 | Batch 2600/2809 | Loss: 1.6523\n",
      "Epoch 1/1 | Batch 2700/2809 | Loss: 1.6313\n",
      "Epoch 1/1 | Batch 2800/2809 | Loss: 1.5420\n",
      "Epoch 1 | Train Loss: 1.8943 | Val Loss: 2.5997\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Hyperparameters (adjust as needed)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 1\n",
    "SEQ_LENGTH = 64  # Must match preprocessing sequence length\n",
    "MODEL_SAVE_PATH = \"/home/itachi/Mini-GPT/models/mini_gpt.pth\"\n",
    "\n",
    "# Load preprocessed data\n",
    "train_data = torch.load(\"/home/itachi/Mini-GPT/data/processed/train_sequences.pt\")\n",
    "val_data = torch.load(\"/home/itachi/Mini-GPT/data/processed/val_sequences.pt\")\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(train_data[\"inputs\"], train_data[\"targets\"])\n",
    "val_dataset = TensorDataset(val_data[\"inputs\"], val_data[\"targets\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MiniGPT(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_len).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Create causal mask\n",
    "def generate_mask(seq_len):\n",
    "    return torch.triu(torch.ones(seq_len, seq_len, device=device) == 1).transpose(0, 1)\n",
    "\n",
    "mask = generate_mask(SEQ_LENGTH)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs, mask=mask)\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevent exploding gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Log progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs, mask=mask)\n",
    "            loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": avg_val_loss,\n",
    "    }, MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Search:\n",
      "<start> The worst of that which gives thee releasing: My bonds in wanofex t my wenomellle wee wne waromeexthele warare belealeat weramo be bllexlat beshat bextin s sthen fononest bexpele bupllin blexelin st \n",
      "\n",
      "Top-k Sampling (k=5):\n",
      "<start> Thee and thy love's might: O let my looks be then the elocenatfrreathemeencanarin thedaruthemeveyexchathindevevelouroffreyouthanouremofrouthearisusthacthinaritous, blinaved, Thed, mpas I dof susthedo\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load vocabulary\n",
    "with open(\"/home/itachi/Mini-GPT/data/processed/vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "    idx_to_char = {int(k): v for k, v in vocab[\"idx_to_char\"].items()}\n",
    "    char_to_idx = vocab[\"char_to_idx\"]\n",
    "\n",
    "# Load trained model\n",
    "MODEL_SAVE_PATH = \"/home/itachi/Mini-GPT/models/mini_gpt.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MiniGPT(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_len).to(device)\n",
    "checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Generate causal mask\n",
    "def generate_mask(seq_len):\n",
    "    return torch.triu(torch.ones(seq_len, seq_len, device=device) == 1).transpose(0, 1)\n",
    "\n",
    "# Decoding strategies\n",
    "def greedy_search(model, seed_text, max_length=100):\n",
    "    input_seq = torch.tensor([char_to_idx[char] for char in seed_text], device=device).unsqueeze(0)\n",
    "    generated_text = seed_text\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq, mask=generate_mask(input_seq.size(1)))\n",
    "            next_token = torch.argmax(output[:, -1, :], dim=-1).item()\n",
    "            generated_text += idx_to_char[next_token]\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def top_k_sampling(model, seed_text, max_length=100, k=5):\n",
    "    input_seq = torch.tensor([char_to_idx[char] for char in seed_text], device=device).unsqueeze(0)\n",
    "    generated_text = seed_text\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq, mask=generate_mask(input_seq.size(1)))\n",
    "            probs = torch.softmax(output[:, -1, :], dim=-1)\n",
    "\n",
    "            # Extract top-k tokens and their probabilities\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, k)\n",
    "\n",
    "            # Normalize top-k probabilities to sum to 1\n",
    "            top_k_probs = top_k_probs / top_k_probs.sum()\n",
    "\n",
    "            # Convert tensors to numpy\n",
    "            top_k_probs = top_k_probs.squeeze().cpu().numpy()\n",
    "            top_k_indices = top_k_indices.squeeze().cpu().numpy()\n",
    "\n",
    "            # Sample next token\n",
    "            next_token = np.random.choice(top_k_indices, p=top_k_probs)\n",
    "\n",
    "            # Append generated character\n",
    "            generated_text += idx_to_char[next_token]\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Generate poetry\n",
    "seed_text = \"<start>\"\n",
    "print(\"Greedy Search:\")\n",
    "print(greedy_search(model, seed_text, max_length=200))\n",
    "print(\"\\nTop-k Sampling (k=5):\")\n",
    "print(top_k_sampling(model, seed_text, max_length=200, k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
