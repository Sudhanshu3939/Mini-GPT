{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Download Shakespeare's Sonnets from Project Gutenberg\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Save the raw text to a file\n",
    "with open(\"/home/itachi/Mini-GPT/data/raw/shakespeare_sonnets.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove all non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "    text = \"<start> \" + text + \" <end>\"       # Add start and end tokens\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Load and clean the text\n",
    "with open(\"/home/itachi/Mini-GPT/data/raw/shakespeare_sonnets.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "cleaned_text = clean_text(raw_text)\n",
    "\n",
    "# Save cleaned text\n",
    "with open(\"/home/itachi/Mini-GPT/data/processed/cleaned_sonnets.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 67, ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Tokenized text: [12, 59, 60, 41, 58, 60, 13, 1, 20, 49, 58, 59, 60, 1, 17, 49, 60, 49, 66, 45, 54, 10, 0, 16, 45, 46, 55, 58, 45, 1, 63, 45, 1, 56, 58, 55, 43, 45, 45, 44, 1, 41, 54, 65, 1, 46, 61, 58, 60, 48], cleaned_text: <start> First Citizen:\n",
      "Before we proceed any furth\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary and tokenizer\n",
    "vocab = sorted(set(cleaned_text))\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}, {vocab}\")\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize(text):\n",
    "    return [char_to_idx[char] for char in text]\n",
    "\n",
    "tokenized_text = tokenize(cleaned_text)\n",
    "print(f\"Tokenized text: {tokenized_text[:50]}, cleaned_text: {cleaned_text[:50]}\")\n",
    "# Save tokenized text and vocabulary\n",
    "import json\n",
    "\n",
    "with open(\"/home/itachi/Mini-GPT/data/processed/tokenized_sonnets.json\", \"w\") as f:\n",
    "    json.dump(tokenized_text, f)\n",
    "\n",
    "with open(\"/home/itachi/Mini-GPT/data/processed/vocab.json\", \"w\") as f:\n",
    "    json.dump({\"char_to_idx\": char_to_idx, \"idx_to_char\": idx_to_char}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the tokenized text\n",
    "train_data, val_data = train_test_split(tokenized_text, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Save the splits\n",
    "with open(\"/home/itachi/Mini-GPT/data/processed/train_data.json\", \"w\") as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(\"/home/itachi/Mini-GPT/data/processed/val_data.json\", \"w\") as f:\n",
    "    json.dump(val_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences: 892262\n",
      "Number of validation sequences: 223018\n",
      "Example training input: [12, 59, 60, 41, 58, 60, 13, 1, 20, 49, 58, 59, 60, 1, 17, 49, 60, 49, 66, 45, 54, 10, 0, 16, 45, 46, 55, 58, 45, 1, 63, 45, 1, 56, 58, 55, 43, 45, 45, 44, 1, 41, 54, 65, 1, 46, 61, 58, 60, 48, 45, 58, 6, 1, 48, 45, 41, 58, 1, 53, 45, 1, 59, 56]\n",
      "Example training target: [59, 60, 41, 58, 60, 13, 1, 20, 49, 58, 59, 60, 1, 17, 49, 60, 49, 66, 45, 54, 10, 0, 16, 45, 46, 55, 58, 45, 1, 63, 45, 1, 56, 58, 55, 43, 45, 45, 44, 1, 41, 54, 65, 1, 46, 61, 58, 60, 48, 45, 58, 6, 1, 48, 45, 41, 58, 1, 53, 45, 1, 59, 56, 45]\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(tokenized_text, seq_length):\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(tokenized_text) - seq_length):\n",
    "        inputs.append(tokenized_text[i:i+seq_length])\n",
    "        targets.append(tokenized_text[i+1:i+seq_length+1])\n",
    "    return inputs, targets\n",
    "\n",
    "# Define sequence length\n",
    "seq_length = 64  # Adjust based on your dataset and memory constraints\n",
    "\n",
    "# Create sequences for training and validation\n",
    "train_inputs, train_targets = create_sequences(train_data, seq_length)\n",
    "val_inputs, val_targets = create_sequences(val_data, seq_length)\n",
    "print(f\"Number of training sequences: {len(train_inputs)}\"), print(f\"Number of validation sequences: {len(val_inputs)}\")\n",
    "print(f\"Example training input: {train_inputs[0]}\"), print(f\"Example training target: {train_targets[0]}\")\n",
    "# Save sequences\n",
    "import torch\n",
    "\n",
    "train_data = {\n",
    "    \"inputs\": torch.tensor(train_inputs, dtype=torch.long),\n",
    "    \"targets\": torch.tensor(train_targets, dtype=torch.long)\n",
    "}\n",
    "val_data = {\n",
    "    \"inputs\": torch.tensor(val_inputs, dtype=torch.long),\n",
    "    \"targets\": torch.tensor(val_targets, dtype=torch.long)\n",
    "}\n",
    "\n",
    "torch.save(train_data, \"/home/itachi/Mini-GPT/data/processed/train_sequences.pt\")\n",
    "torch.save(val_data, \"/home/itachi/Mini-GPT/data/processed/val_sequences.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: <start> First Citizen:\n",
      "Before we proceed any furth\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load tokenized text\n",
    "with open(\"/home/itachi/Mini-GPT/data/processed/tokenized_sonnets.json\", \"r\") as f:\n",
    "    tokenized_text = json.load(f)\n",
    "\n",
    "# Load vocabulary and fix key types\n",
    "with open(\"/home/itachi/Mini-GPT/data/processed/vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "    char_to_idx = vocab[\"char_to_idx\"]\n",
    "    idx_to_char = {int(k): v for k, v in vocab[\"idx_to_char\"].items()}  # Fix keys\n",
    "\n",
    "# Decode a sample sequence\n",
    "sample_tokens = tokenized_text[:50]\n",
    "sample_text = \"\".join([idx_to_char[idx] for idx in sample_tokens])\n",
    "print(\"Sample text:\", sample_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
